{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Autoencoders Applied to Neural Recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper', font_scale=2.0)\n",
    "sns.set_style('ticks')\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import Input, Model, regularizers\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from knn import test_knn\n",
    "\n",
    "SIZES = []\n",
    "RED_DIM = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data setup\n",
    "\n",
    "path = os.getcwd() # put this file in the same dir as the root dir of the training data\n",
    "data_root = os.path.join(path,'tactile-coding')\n",
    "subj_paths = [d for d in os.listdir(data_root) if int(d) <= 12] # only first 12 subjs have EEG data\n",
    "\n",
    "dataset = []\n",
    "for i, subj in enumerate(subj_paths):\n",
    "    subj_path = os.path.join(data_root,subj,'tables')\n",
    "    \n",
    "    # already sorted based on source id --> rows correspond in each\n",
    "    waveforms = pd.read_csv(os.path.join(subj_path,'waveforms.csv'))\n",
    "    units = pd.read_csv(os.path.join(subj_path,'units.csv'))\n",
    "    cell_types = units['cellType']\n",
    "    \n",
    "    # alignment\n",
    "    waveforms['sourceId'] = units['sourceId']\n",
    "    waveforms.set_index('sourceId',inplace = True)\n",
    "    waveforms.columns = waveforms.columns.astype('float')\n",
    "    \n",
    "    waveforms['subj'] = i\n",
    "    data = pd.concat([waveforms,cell_types],axis=1)\n",
    "    dataset += [data]\n",
    "\n",
    "dataset = pd.concat(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### extraction and preprocessing\n",
    "\n",
    "X_df = dataset.drop(['cellType','subj'],axis=1)\n",
    "G_df = dataset['cellType']\n",
    "features = X_df.columns.values\n",
    "\n",
    "# feature scaling to standard normal\n",
    "def standardize(arr):\n",
    "    return (arr - np.mean(arr,axis=1).reshape(-1,1)) / np.std(arr,axis=1).reshape(-1,1)\n",
    "\n",
    "def normalize(arr):\n",
    "    return (arr - arr.min(axis=1).reshape(-1,1)) / (arr.max(axis=1).reshape(-1,1) - arr.min(axis=1).reshape(-1,1))\n",
    "\n",
    "X = normalize(standardize(X_df.values))\n",
    "G = G_df.values\n",
    "\n",
    "y = np.zeros(len(G))\n",
    "cells = np.unique(G)\n",
    "for i, cell in enumerate(cells):\n",
    "    y[G==cell] = i\n",
    "\n",
    "y = y.astype(int)\n",
    "y_cat = to_categorical(y,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this notebook the recordings are normalized between 0 and 1. *For summary stats and preliminary waveform visualizations, see the neural_PCA notebook.* \n",
    "\n",
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### autoencoder builder function\n",
    "\n",
    "def create_ae(init='uniform',activation='relu',optimizer='adam',loss='mean_squared_error',ret_comp=False):\n",
    "    \"\"\"\n",
    "    sizes: list of layer sizes in decreasing order until encoding --> mirrored for decoding\n",
    "    ret_all: returns components of the autoencoder as well if True\n",
    "    \"\"\"\n",
    "    global SIZES\n",
    "    \n",
    "    if len(SIZES) < 3: \n",
    "        raise ValueError(\"Autoencoder must have at least 3 layers.\")\n",
    "    \n",
    "    enc_inp = Input(shape=(SIZES[0],))\n",
    "    enc = enc_inp\n",
    "    for size in SIZES[1:]:\n",
    "        enc = Dense(size,kernel_initializer=init,activation=activation)(enc)\n",
    "        \n",
    "    dec_inp = Input(shape=(SIZES[-1],))\n",
    "    dec = dec_inp\n",
    "    for size in reversed(SIZES[:-1]):\n",
    "        dec = Dense(size,kernel_initializer=init,activation=activation)(dec)\n",
    "    \n",
    "    encoder = Model(enc_inp,enc,name='encoder')\n",
    "    decoder = Model(dec_inp,dec,name='decoder')\n",
    "    autoencoder = Model(enc_inp,decoder(encoder(enc_inp)),name='autoencoder')\n",
    "    \n",
    "    autoencoder.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    \n",
    "    return [encoder, decoder] if ret_comp else autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1296 candidates, totalling 3888 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   39.8s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  7.1min\n",
      "/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3888 out of 3888 | elapsed: 11.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected hyperparameters: 0.666667 using {'activation': 'relu', 'batch_size': 10, 'epochs': 10, 'init': 'uniform', 'loss': 'mean_absolute_error', 'optimizer': 'SGD'}\n"
     ]
    }
   ],
   "source": [
    "### searching for the best hyperparams\n",
    "\"\"\"\n",
    "References: \n",
    "https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "https://stackoverflow.com/questions/49823192/autoencoder-gridsearch-hyperparameter-tuning-keras\n",
    "https://towardsdatascience.com/autoencoders-for-the-compression-of-stock-market-data-28e8c1a2da3e\n",
    "\"\"\"\n",
    "\n",
    "SIZES = [X.shape[1],X.shape[1]//2,X.shape[1]//4,RED_DIM]\n",
    "model = KerasClassifier(build_fn=create_ae,verbose=False)\n",
    "\n",
    "batch_size = [10, 20, 30]\n",
    "epochs = [10, 25, 50]\n",
    "init = ['uniform', 'normal','glorot_uniform','variance_scaling']\n",
    "activation = ['linear','relu','sigmoid','tanh','selu','elu']\n",
    "optimizer = ['SGD', 'Adam','rmsprop']\n",
    "loss = ['mean_squared_error','mean_absolute_error']\n",
    "param_grid = dict(batch_size=batch_size, \n",
    "                  epochs=epochs,\n",
    "                  init=init, \n",
    "                  activation=activation,\n",
    "                  optimizer=optimizer,\n",
    "                  loss=loss\n",
    "                 )\n",
    "\n",
    "search = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3,verbose=True)\n",
    "search_results = search.fit(X, X)\n",
    "score = search_results.best_score_\n",
    "params = search_results.best_params_\n",
    "\n",
    "print(\"\\nSelected hyperparameters: %f using %s\" % (score,params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging this grid search was truly a trial in patience. Scikit has an underlying [bug in its GridSearchCV function](https://github.com/keras-team/keras/issues/13586) that may throw an error on any hyperparameter that modifies the architecture of the network. Here, this bug popped up a few times for layer tuning. So in order to tune the layers and sizes a global parameter needs to be set prior to grid searching. Overall, scikit's grid search is very buggy in tandem with the Keras wrapper; another bug reared when I tried to tune the loss function with cosine similarity. On a lighter note, normalizing the recordings to [0,1] enormously impacted the model, bringing the final grid search accuracy from <50% to 100%. As many guides note, the grid search implementation has underlying randomization so running multiple times and averaging might be a good idea; however, I've found that once a good set of params is available then the search will usually hover around those consistently upon repeat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.7205 - accuracy: 0.0041 - val_loss: 0.7216 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.7172 - accuracy: 0.0000e+00 - val_loss: 0.7183 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.7139 - accuracy: 0.0000e+00 - val_loss: 0.7150 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.7106 - accuracy: 0.0000e+00 - val_loss: 0.7117 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.7072 - accuracy: 0.0000e+00 - val_loss: 0.7084 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.7039 - accuracy: 0.0000e+00 - val_loss: 0.7051 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.7006 - accuracy: 0.0000e+00 - val_loss: 0.7018 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6974 - accuracy: 0.0000e+00 - val_loss: 0.6985 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6941 - accuracy: 0.0000e+00 - val_loss: 0.6952 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6908 - accuracy: 0.0000e+00 - val_loss: 0.6920 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "### fit model with selected params\n",
    "\n",
    "encoder, decoder = create_ae(params['init'],params['activation'],params['optimizer'],ret_comp=True)\n",
    "\n",
    "inp = Input(shape=(SIZES[0],))\n",
    "ae = Model(inp,decoder(encoder(inp)),name='ae')\n",
    "ae.compile(optimizer=params['optimizer'], loss=params['loss'], metrics=['accuracy'])\n",
    "\n",
    "X_tr, X_te = train_test_split(X,test_size=0.1,random_state=42)\n",
    "X_tr, X_val = train_test_split(X_tr,test_size=0.1,random_state=42)\n",
    "history = ae.fit(X_tr, X_tr,\n",
    "                epochs=params['epochs'],\n",
    "                batch_size=params['batch_size'],\n",
    "                shuffle=True,\n",
    "                validation_data=(X_val, X_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
